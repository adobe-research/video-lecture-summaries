 Now I'm going to take quick look at the question what is testing.
 So it's always the case that we have some software under test.
 When we're doing testing, we always require a source of test inputs. This results in test outputs.
 The outputs are processed by an acceptability check.
 If the outputs are okay, then the test case succeeded.
 If the outputs are not okay, then the test case failed and we're going to have to debug.
 This is, of course, really simple.
 On the other hand, a couple of the things on this slide here--that is selecting a good set of test inputs,
 and designing good acceptability test--end up being actually really hard,
 and basically, these are what we are going to be spending this course talking about.
 I would like to give a few facts about testing--by facts of course I mean my own opinions.
 First of all, the goal of testing isn't so much as finding bugs,
 but rather it's finding bugs as early as possible.
 If our goal is just to find some bugs, we go ahead and give the software to our customers
 and let them find the bugs, but of course, there are huge cost associated with doing this.
 What we rather want to do is to move the time in which to find those bugs early.
 And the fundamental reason for that is, is that it's almost always the case
 that the bug that's found earlier is cheaper to fix.
 The second fact is that it's possible to spend a lot of time and effort
 on testing and still do a really bad job.
 Doing testing right requires some imagination and some good taste.
 Third, more testing is not always better.
 In fact, the quality of testing is all about the cost/benefit tradeoff.
 And fundamentally, testing is an economic activity.
 We're spending money or we're spending effort on testing
 in order to save ourselves money and effort later.
 Going along with this, testing methods should be evaluated about the cost per defect found.
 Fourth, testing can be made much easier by designing software to be testable.
 We have a quite a bit more to say about that later. Fifth, quality cannot be tested into software.
 And the corollary of that is--it is surprisingly easy to create software
 but it's impossible to test effectively at all.
 Finally, this is an important one, testing your own software is really hard and there are several reasons
 for this--first of all, it's pretty common for us as developers to be proud of our work.
 What that means is we may not really want our own software to fail.
 This isn't sort of blaming developers--it's rather just a fundamental fact of human nature.
 We're trying to shape software, we're trying to make good software, and we may fundamentally
 at some sort of subconscious level not really want to break it that badly.
 The second reason that testing our own software is hard is that as an individual developer,
 we tend to get our heads buried in the details.
 And we may not know the global context that would allows us to do really effective testing
 of the software that we just wrote.
 The third reason is we can't test the code that we left out
 because we didn't understand that it needed to be written.
 Finally, we can't write effective test cases for parts of the spec that we didn't understand correctly.
 So it's not uncommon of all for us to misunderstand things while reading this verification.
 For example, to keep aside or get something, otherwise, backwards
 and once we've done something like that, what we really have is a bug in our thinking.
 And it's mirrored by the bug in the software, but those kind of bugs are extremely hard for us to find.
 They rarely often takes somebody else testing it to find that kind of bug.
